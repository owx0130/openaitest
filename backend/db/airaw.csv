Top 10 ReadSoft Alternatives;;https://research.aimultiple.com/readsoft-alternatives/;;;;;;ReadSoft is one of the brands that pioneered intelligent accounts payable automation. However, there are new solution providers that claim to offer higher levels of automation. Business leaders are excited by new solutions because because, manual accounts payable processes can be:Complex: Teams span in multiple continents use a multitude of legacy tools to complete mind-numbing tasks like copying numbers from PDF files thousands of times.Expensive: For an enterprise with $1B revenue in annual revenues, accounts payable process is likely to result in $2-3M avoidable cost per year. 1For businesses that want to migrate to a new platform from ReadSoft, there are many alternatives:It can be hard to know which solution to choose. Therefore, we will explore some of the best alternatives to ReadSoft for mid-market and enterprise companies, highlighting their key features and integrations.Table is sorted starting with sponsors. After, companies are sorted by number of reviews.While choosing the top ReadSoft alternatives for our analysis, the following metrics are considered:ReadSoft offers automated solutions for document process automation. Its most widely used application is for invoice automation, where it helps organizations streamline the business processes of receiving, processing, and paying invoices. The company was acquired by Kofax in 2014, and is also known as Kofax AP essentials or Kofax ReadSoft.2Integration: The software can integrate with a variety of ERP systems, allowing the automated transfer of data and facilitating end-to-end invoice processing. (Figure 1) 3Workflow Automation: ReadSoft can automate many of the steps in the invoice validation workflow, reducing the need for manual intervention. For further pros company claims to have, see: Figure 2 4Lacked ability in different fonts. Some users suggest the software might have trouble in process of reading the non-standart fonts. (Figure 4)6Customization Limitations: Some users might find it lacking the flexibility or customization they need for their specific use case.7G2: 2.8/5 based on 6 reviews.8Capterra: N/ATrustRadius: 4.6/5 based on 5 reviews.9Hypatos offers deep learning technology for handling and automating documents. Software provides end-to-end AP automation for enterprises (Figure 5). It can also help with issues such as:Efficiency. Companies miss out on early payment discounts and lose up to 5% of their AP spending on things like duplicate or fraudulent bills that they could have avoided paying for. 10 Hypatos’ AP automation offers to solve situations as such by providing payment reminders, identification of duplicate and fraudulent documents.Every time a user needs to learn something new about a certain type of paper online, they have to annotate the data by hand. These personalized extraction and learning features also aren’t very easy to reuse because they only work for certain data points. Hypatos’ Continual Learning can lessen this effort.Ease of use. 4 out of 10 user reviews in G2 suggests that, the software offers easy implementation. One user summarizes these ideas with a comment about the low learning curve of the software interface.11 (Figure 6)The free trial of the software gives access to limited data.12G2: 4.6/5 based on 10 reviews.13Capterra: 5/5 based on 11 reviews.14TrustRadius: N/ARossum is an AI company that offers a platform for extracting data from invoices and receipts.Reduced manual document entry. Rossum can nearly eliminate manual data entry. A case study with Kiwi suggest that, Rossum contributed to Kiwi’s scalability, by eliminating the need for error-prone and time consuming process of manual PDF entry. (Figure 7) Kiwi’s accounts payable staff used Rossum to process invoices. Moreover, they used software’s interface to pair vendors and cost center records.15Ease of Integration: Rossum can typically integrate with various ERP and accounting systems. This reduces the need for significant changes in a company’s IT structure. 13 reviews on G2 shows that users are happy with the feature.16 (Figure 8)Insufficient data extraction. Customer experiences show that Rossum can face trouble in extracting data from different kinds of documents.17Language issues. Several customer experiences display that Rossum cannot recognize different languages in all cases. (Figure 9)G2: 4.4/5 based on 70 reviews.18Capterra: 4.6/5 based on 10 reviews. 19TrustRadius: N/AMedius is a leading global provider of cloud-based spend management solutions such as automating accounts payable, e-procurement, and other related services.Frees up time for accounts payable teams. Møbelringen, a retail company was operating with a system based on paper. Their AP team had to match bills to POs by hand. Since about 80% of bills were based on POs, this process was too long. Medius’ accounts payable automation solution eliminated paper-based system and increased the touchless rate which freed AP team’s time.20 (Figure 10)Increased visibility. Medius by allowing audit trails and month-end accruals, allows managers to track invoices better. In that terms, the software helped Lush, a cosmetics company that was having trouble to keep the track of the invoices monthly.21 (Figure 11)Slow-downs. In instances where there are large-volume of payments (Figure 12) users complained that the software slows down significantly.22Complex functions. Users noted that the software can become overwhelming when using functions such as search function, reporting and uploading complex details.23Quadient Accounts Payable Automation by Beanworks is a platform that can automate and streamline accounts payable processes. Beanworks, acquired by Quadient in March 2021, was the first company to build the platform.24Prevents Duplicates. Based on 18 reviews, users agreed that the software can prevent duplicate payments. 25(Figure 13)Integration. 61 reviews points out that the Quadient offers wide scale of integration with other accounting and financial management softwares.26(Figure 14)Mobile use. Software offers mobile solutions, which carries features such as parsing and capturing receipts.27 (Figure 15)Unability to handle large numbers. The software is able to handle 80 invoices per page.28 Large organizations and enterprises might find this number insufficient.29 (Figure 16)Inefficiencies in approval workflow. When changes are necessary in approval channels, users might have to handle them one by one manually.30 Making changes can be time-consuming in approval channels also because one-confirmation might lead all the way back to beginning point on the interface.31 (Figure 17)G2: 4.4/5 based on 66 reviews.32Capterra: 4.6/5 based on 40 reviews 33TrustRadius: 4.6/5 based on 6 reviews 34Hyland’s OnBase software offers services to automate Accounts Payable (AP) processes and helps accounts payable department workflows, cut down on mistakes, and make it more efficient.Efficiency. A case study on Canal Barge shows that, OnBase helped company improve efficiency in areas of invoice tracking, visibility, approval. Software offers services such as automatic notification when there is an invoice to approve and automatic capture of invoice data.35 (Figure 18)Increased Capacity & Reduced Costs. A case study onNorth American Mining Company reveals that, with the use of OnBase, the AP department was able to handle higher volumes of invoices with fewer human-resource. When merged with improvement in visibility, the company was able to identify and leverage early-payment discounts, and save $5 million annually.36Support. At least 7 userssuggested that software lack both in-use and customer support. The response times of customer support is criticized.37 (Figure 19)Complex. At least 5 reviews on G2 reveals that in terms of licensing, configuration modules and setup, the software can be quite complex.38G2: 4.3/5 based on 168 reviews.39Capterra: 4.3/5 based on 73 reviews.40TrustRadius: 3.9/5 based on 85 reviews.41Numerous tech firms, such as Hypatos, are sponsors of AIMultiple.If you have further questions regarding the topic, reach out to us: Burak is an Industry Analyst in AIMultiple. He received his Masters' degree in Political Science from Middle East Technical University. He has background in researching location-based platforms.
LIMA: Less Is More for Alignment - The paper that will shake up AI [Breakdowns];;https://artificialintelligencemadesimple.substack.com/p/lima-less-is-more-for-alignment-the;;;;;;Hey, it’s Devansh 👋👋In my series Breakdowns, I go through complicated literature on Machine Learning to extract the most valuable insights. Expect concise, jargon-free, but still useful analysis aimed at helping you understand the intricacies of Cutting Edge AI Research and the applications of Deep Learning at the highest level.If you’d like to support my writing, please consider buying and rating my 1 Dollar Ebook on Amazon or becoming a premium subscriber to my sister publication Tech Made Simple using the button below.Help me buy chocolate milkp.s. you can learn more about the paid plan here. If your company is looking to build AI Products or Platforms and is looking for consultancy for the same, my company is open to helping more clients. Message me using LinkedIn, by replying to this email, or on the social media links at the end of the article to discuss your needs and see if you’d be a good match. We have continued to see the development of more Large Language Models. Falcon has landed at HuggingFace Hub and models like Orca have shaken up the rankings. Amongst these developments, was a very interesting paper I came across- Textbooks Are All You Need- by the people at Microsoft Research. Their approach and results are worth paying attention to- We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval. This made me think back to many of the conversations, I’ve heard. A lot of engineers I’ve spoken with assume that since the scale in training these large language models is so large, meaningful finetuning should be of a similar scale, especially in more complex fields like coding, law, and finance. However, this is far from the truth. Multiple research papers across various tasks have shown that intelligently selecting a few samples can greatly augment the performance of large baseline models without adding too much to the cost. Such vastly superior scaling would mean that we could go from 3% to 2% error by only adding a few carefully chosen training examples, rather than collecting 10x more random ones-Beyond neural scaling laws: beating power law scaling via data pruning. We broke this paper down hereIn the context of Large Language Models, this phenomenon was shown beautifully by the paper-  LIMA: Less Is More for Alignment. The authors of LIMA took a baby model- 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. However, this baby managed to punch above its weight- LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback.In this article, we will touch upon some of the results, go over why Less is More when it comes to alignment, and also cover the Superficial Alignment Hypothesis. To end, we will touch on one of the biggest weaknesses of this approach- its fragility to adversarial inputs. While the underlying concept in this paper should be fairly obvious- the results do warrant a deeper look. To fully understand the core ideas of this idea- we must first understand the Superficial Alignment Hypothesis and the relative importance of pretraining vs alignment for the knowledge of any given Large Language Model. The authors open with an interesting hypothesis about LLMs and whether they actually learn from finetuning- called the Superficial Alignment Hypothesis. Put simply, SAH posits that LLMs learn during the pretraining phase and the best utilization of the alignment phase (RLHF, Supervised Learning, Finetuning for specific outputs etc) is not to provide our AI Models with knowledge but to tune their output to match our needs. In the words of the authors- A model’s knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users.Taken a step further, this tells us that we don’t really need too many samples and one could sufficiently tune a pretrained language model with a rather small set of examples. This is what sets up the basis for LIMA- where the authors use 1000 carefully selected samples to tune their model, as opposed to the more complex procedures implemented by the household LLMs. But how well do they actually do? Time to find out- So how do we compare language models and their performance?  This is a trickier question than you would think. Keep in mind, LLMs are very good at generating text that looks correct (is most syntactically correct)- so using rule-based checks is meaningless. Take a second on how you’d set up your quality checks before proceeding with this article. Thinking about such things is crucial in developing your skills in setting up ML experiments. For the purposes of this paper, it is important to zoom out and look at their evaluation setup as a whole. This is crucial for contextualizing the results of the experiments (one of the biggest mistakes you can make is to blindly look through the results of such papers w/o understanding the experiment setup). The evaluation pipeline has the following important features- Generation- “For each prompt, we generate a single response from each baseline model using nucleus sampling. We apply a repetition penalty of previously generated tokens with a hyperparameter of 1.2. We limit the maximum token length to 2048.”Methodology- At each step, we present annotators with a single prompt and two possible responses, generated by different models. The annotators are asked to label which response was better, or whether neither response was significantly better than the other (look at the image below).  This is supplemented by the use of GPT4 for annotations. Below is a sample prompt used to accomplish that. The evaluation uses a 6-scale Likert score. The placeholders "task" and "submission" will be replaced by specific details from the actual case being evaluatedInter-Annotator Agreement- “We compute inter-annotator agreement using tie-discounted accuracy: we assign one point if both annotators agreed, half a point if either annotator (but not both) labeled a tie, and zero points otherwise. We measure agreement over a shared set of 50 annotation examples (single prompt, two model responses – all chosen randomly), comparing author, crowd, and GPT-4 annotations. Among human annotators, we find the following agreement scores: crowd-crowd 82%, crowd-author 81%, and author-author 78%. Despite some degree of subjectivity in this task, there is decent agreement among human annotators” This should now give you the context for evaluating the results, and seeing how LLM comparisons are done (they can be a bit hacky- which means that there is a great market for strong benchmarks. If any of you are looking for a business idea in AI, developing benchmarks is a great field to get into). Let’s now look at the raw numbers- Out of 50 test prompts, 50% of LIMA answers are considered excellent, and only 12% of outputs are designated as fail. There is no  notable trend within the failure cases. Most impressive is how it deals with out-of-distribution prompts: prompts where the task is unrelated to the format of the 1000 training samples. We see that of the 20 OOD prompts- 20% of responses fail, 35% pass, and 45% are excellent. Based on this this is a small sample, it appears that LIMA  is able to generalize well. This is a great start and highlights the potential for LIMA as a standalone solution. But how does it compare to other models? As mentioned at the start of this article, it does very well. To reiterate- Our first observation is that, despite training on 52 times more data, Alpaca 65B tends to produce less preferable outputs than LIMA. The same is true for DaVinci003, though to a lesser extent; what is striking about this result is the fact that DaVinci003 was trained with RLHF, a supposedly superior alignment method. Bard shows the opposite trend to DaVinci003, producing better responses than LIMA 42% of the time; however, this also means that 58% of the time the LIMA response was at least as good as Bard. Finally, we see that while Claude and GPT-4 generally perform better than LIMA, there is a non-trivial amount of cases where LIMA does actually produce better responses. Perhaps ironically, even GPT-4 prefers LIMA outputs over its own 19% of the time.-In cases where costs are valuable, the higher returns of the bigger models might not be worth the additional burden they put on your system. Through their work, the authors note that LIMA seems to struggle with text that is structurally more complex. “In our preliminary experiments, we find that although LIMA can respond to many questions in our development set well, it cannot consistently respond to questions that specify the structures of the answer well, e.g. summarizing an article into bullet points or writing an article consisting of several key elements.” Although this can be remedied with more training (up to a point), as shown by the graph below. Adding some of these formatted outputs seems to give LIMA some level of generalization superpowers. “We added six examples with various formatting constraints, such as generating a product page that includes Highlights, About the Product, and How to Use or generating question-answer pairs based on a given article. After training with these six additional examples, we test the model on a few questions with format constraints and observe that LIMA responses greatly improve. We present two examples in Figure 13, from which we can see that LIMA fails to generate proper answers without structure-oriented training examples (left column), but it can generate remarkably complex responses such as a marketing plan even though we do not have any marketing plan examples in our data (right column).” To round off the LIMA results evaluation, let’s take a look at the results of the ablation studies conducted by the authors. To those of you not familiar with the term- An ablation study investigates the performance of an AI system by removing certain components to understand the contribution of the component to the overall system. The authors wanted to evaluate what role roles data diversity, quality, and quantity played in Alignment. Here is a quick summary of their results- TL;DR- We observe that, for the purpose of alignment, scaling up input diversity and output quality have measurable positive effects, while scaling up quantity alone might not. Who would have seen this coming? I have an entire article where I countered Sam Altman’s claim that there was an era for scaling up models/training. Originally, I received some pushback, but more research is coming to a similar conclusion. Diversity- To test the effects of prompt diversity, while controlling for quality and quantity, we compare the effect of training on quality-filtered Stack Exchange data, which has heterogeneous prompts with excellent responses, and wikiHow data, which has homogeneous prompts with excellent responses. While we compare Stack Exchange with wikiHow as a proxy for diversity, we acknowledge that there may be other conflating factors when sampling data from two different sources. We sample 2,000 training examples from each source (following the same protocol from Section 2.1). Figure 5 shows that the more diverse Stack Exchange data yields significantly higher performance.Quality- To test the effects of response quality, we sample 2,000 examples from Stack Exchange without any quality or stylistic filters, and compare a model trained on this dataset to the one trained on our filtered dataset. Figure 5 shows that there is a significant 0.5-point difference between models trained on the filtered and unfiltered data sources.Quantity- Scaling up the number of examples is a well-known strategy for improving performance in many machine learning settings. To test its effect on our setting, we sample exponentially increasing training sets from Stack Exchange. Figure 6 shows that, surprisingly, doubling the training set does not improve response quality. This result, alongside our other findings in this section, suggests that the scaling laws of alignment are not necessarily subject to quantity alone, but rather a function of prompt diversity while maintaining high quality responses.That Hall of Fame-worthy last sentence goes against everything that cutting-edge AI Research has been about. I’m sure you’re as shocked as I am, but I need to you to focus now. We’re about to end with a discussion a very important discussion wrt to a possible drawback of this approach.  The results demonstrated by LIMA open the door for people who want to tune LLMs for their specific needs but have limited computing to do so. However, this efficiency comes at a cost. In the words of the authors- LIMA is not as robust as product-grade models; while LIMA typically generates good responses, an unlucky sample during decoding or an adversarial prompt can often lead to a weak response. While LIMA might seem like a magic bullet, it comes with a huge security risk: its vulnerability to adversarial suggestion. To understand why this is the case, it’s important to understand the LIMA approach. At its core, LIMA is about efficiency- using a few high-quality samples to teach the model the general outline of a particular task. So why is this bad for security? Simply put adversarial prompts are risky precisely because they also match the general outline well. The adversarial devil dwells in the details (my English teachers would be so proud of that alliteration), and teaching AI to deal with all the possible hazards would overwrite all the efficiency benefits (keep in mind even beefy models like GPT can be broken easily). So how do we deal with this? The most important step is to use your judgment. LIMA (and most of the popular GenAI services) are best used when safety isn’t a huge deal: a customer service bot can for the most bit get a few things off. It’s not critical, and there’s no huge loss. Similarly, if you’re using a bot to quickly transcribe notes into a particular format or store data in a specific way then you can keep things simple and use this approach for great ROI. If you’re looking for a more robust solution for information retrieval and synthesis, my go-to is to use retrieval-based architectures. These are very good for parsing through documents/knowledge bases to extract solutions, and I would highly recommend looking at these instead of trying to fine-tune GPT on your data (which is extremely inefficient and will still have problems). A simple retrieval-based architecture is shown below. For a more complex version, I would suggest taking a look at Sphere by Meta, which I broke down here. If you are looking for someone to help you build solutions based on these architectures (or are looking to integrate AI/Tech into your solutions in any other way), then feel free to reach out to me and we can discuss your needs. At its core, the LIMA approach leads to a glass cannon- it works well but will be more fragile than using more samples (which would encode robustness more).  However, despite its fragility, LIMA is game-changing for AI. Combined with Open Source LLMs, LIMA can be used to create very convincing proof of concepts and demos, which will help democratize ML further. People from differing backgrounds and with lower access to resources can still create compelling alpha-stage products, which will improve the accessibility of AI and ML Entrepreneurship (something the field desperately needs). In a field where everyone is losing it over gargantuan models and obscene amounts of training data- LIMA is a welcome (even if somewhat obvious) contribution. That is it for this piece. I appreciate your time. As always, if you’re interested in working with me or checking out my other work, my links will be at the end of this email/post. If you like my writing, I would really appreciate an anonymous testimonial. You can drop it here. And if you found value in this write-up, I would appreciate you sharing it with more people. It is word-of-mouth referrals like yours that help me grow.Use the links below to check out my other content, learn more about tutoring, reach out to me about projects, or just to say hi.Small Snippets about Tech, AI and Machine Learning over hereAI Newsletter- https://artificialintelligencemadesimple.substack.com/My grandma’s favorite Tech Newsletter- https://codinginterviewsmadesimple.substack.com/Check out my other articles on Medium. : https://rb.gy/zn1aiuMy YouTube: https://rb.gy/88iwddReach out to me on LinkedIn. Let’s connect: https://rb.gy/m5ok2yMy Instagram: https://rb.gy/gmvuy9My Twitter: https://twitter.com/Machine01776819 No postsReady for more?
ChatGPT: A Bullshit Tool For Bullshit Jobs;;https://thealgorithmicbridge.substack.com/p/chatgpt-a-bullshit-tool-for-bullshit;;;;;;ChatGPT is a bullshit generator.You’ve heard that before, haven’t you? Well, unlike probably all the other times someone’s said it, I don’t mean it as a put-down but as a compliment.“Bullshit generator” suits ChatGPT because its output doesn’t hold any “regard for truth.” ChatGPT doesn't lie, which is an intentional attempt to conceal the truth. It generates text that happens to be either right or wrong—but only incidentally.Whatever you need it to say, it will oblige with the appropriate prompt.I used to think that was bad.Why do we want such a shapeless, opinion-less tool, you may ask. A tool that can't differentiate between real stuff and the vast array of imaginary alternatives. A tool that can learn anything, possible and impossible, and will inadvertently and spontaneously confabulate facts about the world.Well, I no longer think that’s bad. It turns out this handy property—automated bullshitting—is singularly useful nowadays.ChatGPT may not be the end of meaning, as I've often wondered, but quite the opposite: The end of meaninglessness.Let's see why.Douglas Hofstadter has been giving mixed comments about ChatGPT and GPT-4 recently.Two weeks ago he publicly admitted to being terrified that AI will eventually eclipse us at everything (a fear that apparently goes back a decade). Yet a few days ago he wrote:“I frankly am baffled by the allure, for so many unquestionably insightful people (including many friends of mine), of letting opaque computational systems perform intellectual tasks for them. Of course it makes sense to let a computer do obviously mechanical tasks, such as computations, but when it comes to using language in a sensitive manner and talking about real-life situations where the distinction between truth and falsity and between genuineness and fakeness is absolutely crucial, to me it makes no sense whatsoever to let the artificial voice of a chatbot, chatting randomly away at dazzling speed, replace the far slower but authentic and reflective voice of a thinking, living human being.”In trying to find coherence in his statements, I posted the above excerpt on Twitter. Interestingly, no one pointed out Hofstadter’s conflicting views. Instead, some suggested he may be too disconnected from the struggles of common people.Accordingly, the most liked response was this:“The problem is that most of us don't get to live in a purely thoughtful, intellectual environment. Most of us have to live with jobs where we’re required to write corporate nonsense in response to corporate nonsense. Automating this process is an attempt to recapture some sanity.”I kinda agreed with this more quotidian, relatable perspective—who wouldn't use ChatGPT to lift the burden of empty and pointless work?—yet Hofstadter’s comment feels right to me. But after a day of thinking, the apparent contradiction disappeared:Like Hofstadter, I live in a somewhat “purely thoughtful, intellectual environment,” abstracted from the emptiness of “corporate nonsense.” My professional career has been an incessant effort to not be absorbed into it. That’s why I never really saw the need to use ChatGPT. That’s why I couldn’t understand just how useful—life-saving even—it is for so many people.Now I get it: ChatGPT allows them to escape what I've been avoiding my whole life. People are just trying to “recapture some sanity” with the tools at their disposal as I do when I write. Whereas for me, as a blogger-analyst-essayist, ChatGPT feels like an abomination, for them—for most of you—it couldn't be more welcome.Anthropologist David Graeber coined the term “bullshit jobs” in 2013. He wrote:“…technology has been marshaled … to figure out ways to make us all work more. In order to achieve this, jobs have had to be created that are, effectively, pointless … The moral and spiritual damage that comes from this situation is profound. It is a scar across our collective soul.”He continues:“The ruling class has figured out that a happy and productive population with free time on their hands is a mortal danger … And, on the other hand, the feeling that work is a moral value in itself, and that anyone not willing to submit themselves to some kind of intense work discipline for most of their waking hours deserves nothing, is extraordinarily convenient for them.”It’s hard to say objectively which work has value and which doesn’t, but it’s easy for each of us to answer this question: Do you spend time on tasks you don’t like, that you believe don’t really need to be done?“This is a profound psychological violence here. How can one even begin to speak of dignity in labour when one secretly feels one's job should not exist? How can it not create a sense of deep rage and resentment.”People should be able to work 4-hour days, as Graeber thinks. But the system won’t allow that. That’s exactly why ChatGPT has found such widespread adoption among bullshit workers (and students).The question was never “why do people choose to use ChatGPT?” as Hofstadter implies, but “why do people feel the need to do so?”The answer is evident now: What else but a bullshit-generating tool to cancel out bullshit-requiring tasks so people can finally fill their lives with something else?No one wants to be a bullshiter all day long yet many people have to. ChatGPT can’t help but be one. The match couldn’t be more perfect. ChatGPT isn't emptying their professional lives of meaning. No, it’s emptying them of the modern illness of meaninglessness.ChatGPT is, intentionally, the best AI tool ever created—thanks to years of AI research and development—and, unintentionally, the tool that best fits one of the deepest and more widespread needs of our generation. It is a twofold success and a win-win for people who can automate the bullshit dimension of their jobs.ChatGPT can be rightfully criticized for many things: How OpenAI gathered all the data without permission or compensation; how the model was fine-tuned by exploiting factory-like workers from poorer countries; how it’s been deemed a milestone toward AGI without giving any details about its architecture or training data (we now have a better idea); and how it’s been implicitly marketed as somewhat foolproof when it’s more akin to a bullshit generator.But I have a newfound appreciation and respect for people who, in an attempt to recapture their sanity by avoiding becoming victims of the corporate nonsense that pains our times, are exploiting ChatGPT to the extent of their ability without letting anyone know.I don’t think there’s a more honorable and valuable use of AI than setting oneself free.Technology was “marshaled” to fill up our time with meaningless tasks, as Graeber pointed out. Ironically, it has now provided a tool to do those tasks for us.A bullshit tool for bullshit jobs.Now, where do we go from here?Leave a commentThis is precisely what I use ChatGPT for. At my job at a government adjacent private company, we have to write many reports purely for procedural reasons. These reports are often not very informative and could be easily be reduced to 5 pages, but they are often 30+. ChatGPT rules for generating the jargon filled prose necessary to complete them. What % of ChatGPT users do you think are using it for “bullshitting” purposes?Do you think there are other common use cases?No postsReady for more?
